{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing the progress of neural network training with Tensorboard\n",
    "\n",
    "There are many hyperparameters to tune when you're trying to improve the performance of a neural network. There are also many output measures during the training process. Keep track of all these numbers is a very challenging task. Fortunately, the good folks at Google knew this already and built [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) exactly for this purpose. \n",
    "\n",
    "In this project, we'll use CIFAR-10 dataset and a simple convolutional neural network to learn how to use Tensorboard to keep track of and visualize parameters you're interested in during the training process. We'll also learn how to better understand a computation graph using Tensorboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get the training data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    if not isfile(tar_gz_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "            urlretrieve('https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', tar_gz_path, pbar.hook)\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "### Normalize\n",
    "In the cell below, we implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x = x.astype(np.float32)\n",
    "    return ( x - x.min() ) / ( x.max() - x.min() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Here we implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    output = np.zeros([len(x), 10])   \n",
    "    for idx, item in enumerate(x):\n",
    "        output[idx, item] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "\n",
    "\n",
    "### Input\n",
    "\n",
    "Here, we're using [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope) to group the nodes within a single name scope. All the nodes in the same name scope will appear as a group when you later visualize the computation graph in Tensorboard. Note that every time you use `tf.name_scope`, you'll create a new one even when you're trying to use the same name. So for example, if you do `with tf.name_scope('inputs'):` twice, you'll actually create two name scopes. The first one is 'inputs', the second one is 'inputs_1'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_input(image_shape, n_classes):\n",
    "    # Grouping nodes into a single name scope in the computation graph.\n",
    "    with tf.name_scope('inputs'):\n",
    "        x = tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], \"x\")\n",
    "        y = tf.placeholder(tf.float32, [None, n_classes], \"y\")\n",
    "        keep_prob = tf.placeholder(tf.float32, None, \"keep_prob\")\n",
    "        learning_rate = tf.placeholder(tf.float32, None, \"lr\")\n",
    "    return x, y, keep_prob, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Here we're using to [tf.summary](https://www.tensorflow.org/api_docs/python/tf/summary) to keep track of parameters we're interested in. `tf.summary` writes the value of the parameters to protocol buffer, which can later be written into a log file using [tf.summary.FileWriter](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter) and viewed in Tensorboard. Here, since weights and biases are not just single scalars, so we use histogram to keep track of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    wc1 = tf.Variable(tf.truncated_normal( [ conv_ksize[0], conv_ksize[1], x_tensor.shape[3].value, conv_num_outputs ],\n",
    "                                      mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    bc1 = tf.Variable(tf.truncated_normal([conv_num_outputs],\n",
    "                                      mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    \n",
    "    x_out = tf.nn.conv2d(x_tensor, wc1, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    x_out = tf.nn.bias_add(x_out, bc1)\n",
    "    \n",
    "    x_out = tf.nn.relu(x_out)\n",
    "    x_out = tf.nn.max_pool(x_out, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                           strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    # Keeping track of weights and biases\n",
    "    tf.summary.histogram('weights', wc1)\n",
    "    tf.summary.histogram('biases', bc1)\n",
    "    return tf.nn.relu(x_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    return tf.reshape(x_tensor, [tf.shape(x_tensor)[0], np.prod(x_tensor.shape[1:]).value])\n",
    "    #return tf.reshape(x_tensor, [-1, np.prod(x_tensor.shape[1:]).value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    fc = tf.reshape(x_tensor, [-1, np.prod(x_tensor.shape[1:]).value])\n",
    "    w = tf.Variable(tf.truncated_normal([np.prod(x_tensor.shape[1:]).value, num_outputs], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    b = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "    tf.summary.histogram('weights', w)\n",
    "    tf.summary.histogram('biases', b)\n",
    "    return tf.nn.relu(tf.add(tf.matmul(fc, w), b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    w = tf.Variable(tf.random_normal([x_tensor.shape[1].value, num_outputs]))\n",
    "    b = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    return tf.add(tf.matmul(x_tensor, w), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \n",
    "    with tf.name_scope(\"CNN\"):\n",
    "        with tf.variable_scope(\"conv1\"):\n",
    "            conv1 = conv2d_maxpool(x, 10, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "        with tf.variable_scope(\"conv2\"):\n",
    "            conv2 = conv2d_maxpool(conv1, 20, (5, 5), (1, 1), (2, 2), (2, 2))\n",
    "        with tf.variable_scope(\"conv3\"):\n",
    "            conv3 = conv2d_maxpool(conv2, 30, (7, 7), (1, 1), (2, 2), (2, 2))\n",
    "\n",
    "        # TODO: Apply a Flatten Layer\n",
    "        f = flatten(conv3)\n",
    "\n",
    "        # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "        with tf.variable_scope(\"fc1\"):\n",
    "            fc1 = fully_conn(f, 100)\n",
    "            fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "        with tf.variable_scope(\"fc2\"):\n",
    "            fc2 = fully_conn(fc1, 50)\n",
    "            fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "        with tf.variable_scope(\"fc3\"):\n",
    "            fc3 = fully_conn(fc2, 20)\n",
    "\n",
    "        o = output(fc2, 10)\n",
    "    \n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a training op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_op():\n",
    "    \n",
    "    # Remove previous weights, bias, inputs, etc..\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Inputs\n",
    "    x, y, keep_prob, lr = neural_net_input((32,32,3), 10)\n",
    "    \n",
    "    # Model\n",
    "    logits = conv_net(x, keep_prob)\n",
    "\n",
    "    # Name logits Tensor, so that is can be loaded from disk after training\n",
    "    with tf.name_scope('logits'):\n",
    "        logits = tf.identity(logits, name='logits')\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name='cost')\n",
    "    loss_summ = tf.summary.scalar('loss', cost)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('predictions'):\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    accuracy_summ = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    #summary = tf.summary.merge([loss_summ, accuracy_summ], name='summary')\n",
    "    #Alternatively, you can use tf.summary.merge_all()\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    return x, y, keep_prob, lr, summary, optimizer, cost, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.106800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3067 Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.3144 Validation Accuracy: 0.094600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.3099 Validation Accuracy: 0.099800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3102 Validation Accuracy: 0.097800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3044 Validation Accuracy: 0.097800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.3086 Validation Accuracy: 0.094200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.3144 Validation Accuracy: 0.094600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.3102 Validation Accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.3100 Validation Accuracy: 0.097800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.3046 Validation Accuracy: 0.105000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.3091 Validation Accuracy: 0.094200\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.3143 Validation Accuracy: 0.094600\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.3100 Validation Accuracy: 0.097800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.3047 Validation Accuracy: 0.105000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.3093 Validation Accuracy: 0.094200\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.3142 Validation Accuracy: 0.094200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.3100 Validation Accuracy: 0.097800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.3094 Validation Accuracy: 0.094200\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     2.3095 Validation Accuracy: 0.094200\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     2.3095 Validation Accuracy: 0.094200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     2.3048 Validation Accuracy: 0.105000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     2.3096 Validation Accuracy: 0.094200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     2.3141 Validation Accuracy: 0.094200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     2.3103 Validation Accuracy: 0.099800\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     2.3099 Validation Accuracy: 0.097800\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2460 Validation Accuracy: 0.151800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2233 Validation Accuracy: 0.164000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2077 Validation Accuracy: 0.160400\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.0962 Validation Accuracy: 0.189600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.0476 Validation Accuracy: 0.199600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0410 Validation Accuracy: 0.237800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.9919 Validation Accuracy: 0.260800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.0080 Validation Accuracy: 0.254200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.0333 Validation Accuracy: 0.257000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.9486 Validation Accuracy: 0.283200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9329 Validation Accuracy: 0.286800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.9440 Validation Accuracy: 0.288800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.9400 Validation Accuracy: 0.287400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.9833 Validation Accuracy: 0.269400\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.9145 Validation Accuracy: 0.291800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9317 Validation Accuracy: 0.275200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.9000 Validation Accuracy: 0.311200\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.9333 Validation Accuracy: 0.280800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.9626 Validation Accuracy: 0.261000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.9301 Validation Accuracy: 0.293200\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9146 Validation Accuracy: 0.300600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.9016 Validation Accuracy: 0.314400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.8938 Validation Accuracy: 0.300800\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.9226 Validation Accuracy: 0.298000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.9146 Validation Accuracy: 0.298200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9166 Validation Accuracy: 0.288200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.8946 Validation Accuracy: 0.309400\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.9113 Validation Accuracy: 0.308000\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.9318 Validation Accuracy: 0.296600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.8951 Validation Accuracy: 0.309600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.8928 Validation Accuracy: 0.311600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.8908 Validation Accuracy: 0.301800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.8797 Validation Accuracy: 0.312400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.8844 Validation Accuracy: 0.308600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.8890 Validation Accuracy: 0.303600\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.9014 Validation Accuracy: 0.299800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.8752 Validation Accuracy: 0.313400\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.8785 Validation Accuracy: 0.311000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.9066 Validation Accuracy: 0.308800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.8976 Validation Accuracy: 0.318000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.8782 Validation Accuracy: 0.301800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.8856 Validation Accuracy: 0.310400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.9370 Validation Accuracy: 0.293200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.8859 Validation Accuracy: 0.311600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.8762 Validation Accuracy: 0.308400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.8982 Validation Accuracy: 0.296600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.8920 Validation Accuracy: 0.311400\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.8706 Validation Accuracy: 0.312600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.8685 Validation Accuracy: 0.317600\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.9232 Validation Accuracy: 0.300400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.8769 Validation Accuracy: 0.301600\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.8735 Validation Accuracy: 0.320200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.8873 Validation Accuracy: 0.319400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.8805 Validation Accuracy: 0.302600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.8660 Validation Accuracy: 0.305800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.8737 Validation Accuracy: 0.303000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.8636 Validation Accuracy: 0.314800\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.8770 Validation Accuracy: 0.291000\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.8699 Validation Accuracy: 0.312400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.8996 Validation Accuracy: 0.310200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.8881 Validation Accuracy: 0.315400\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.8892 Validation Accuracy: 0.306000\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.8682 Validation Accuracy: 0.305200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.9014 Validation Accuracy: 0.304400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.8627 Validation Accuracy: 0.327800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.8899 Validation Accuracy: 0.318400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.8647 Validation Accuracy: 0.320200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.8633 Validation Accuracy: 0.316200\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.9432 Validation Accuracy: 0.296400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.8649 Validation Accuracy: 0.313800\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.8620 Validation Accuracy: 0.305600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.8844 Validation Accuracy: 0.308600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.8701 Validation Accuracy: 0.294800\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.9079 Validation Accuracy: 0.299600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.9100 Validation Accuracy: 0.304000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.8747 Validation Accuracy: 0.316400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.8868 Validation Accuracy: 0.311400\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.8738 Validation Accuracy: 0.292600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.8760 Validation Accuracy: 0.312400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.8832 Validation Accuracy: 0.310000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.8827 Validation Accuracy: 0.314600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.9406 Validation Accuracy: 0.309200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.8631 Validation Accuracy: 0.312000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.8961 Validation Accuracy: 0.303400\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.8714 Validation Accuracy: 0.316400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.9286 Validation Accuracy: 0.297000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.8474 Validation Accuracy: 0.321600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.8821 Validation Accuracy: 0.304200\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.8803 Validation Accuracy: 0.316600\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.8657 Validation Accuracy: 0.303800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.8832 Validation Accuracy: 0.314400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.8519 Validation Accuracy: 0.320600\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.8668 Validation Accuracy: 0.283200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.8945 Validation Accuracy: 0.303200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.8622 Validation Accuracy: 0.321000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.8856 Validation Accuracy: 0.321200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.8729 Validation Accuracy: 0.318000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.8720 Validation Accuracy: 0.302000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.9121 Validation Accuracy: 0.309000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.8687 Validation Accuracy: 0.317200\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.5505 Validation Accuracy: 0.122800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3138 Validation Accuracy: 0.179600\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1525 Validation Accuracy: 0.206600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.9579 Validation Accuracy: 0.285800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.8237 Validation Accuracy: 0.349200\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.7554 Validation Accuracy: 0.372400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.6812 Validation Accuracy: 0.409800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.6857 Validation Accuracy: 0.390600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.5758 Validation Accuracy: 0.444000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.5568 Validation Accuracy: 0.469600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.5884 Validation Accuracy: 0.437800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.4636 Validation Accuracy: 0.489400\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.4341 Validation Accuracy: 0.492400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.4046 Validation Accuracy: 0.512800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.4012 Validation Accuracy: 0.510800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.4278 Validation Accuracy: 0.488000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.3500 Validation Accuracy: 0.525200\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.3236 Validation Accuracy: 0.537200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.3062 Validation Accuracy: 0.552600\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.3343 Validation Accuracy: 0.536000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.3639 Validation Accuracy: 0.526400\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.2486 Validation Accuracy: 0.567000\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.3512 Validation Accuracy: 0.538200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.2129 Validation Accuracy: 0.577600\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.2260 Validation Accuracy: 0.583200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.2135 Validation Accuracy: 0.581000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.2165 Validation Accuracy: 0.579600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.1981 Validation Accuracy: 0.590800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.1786 Validation Accuracy: 0.591200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.1405 Validation Accuracy: 0.611000\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.1637 Validation Accuracy: 0.589000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.1495 Validation Accuracy: 0.605000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.1299 Validation Accuracy: 0.611400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.1305 Validation Accuracy: 0.607800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.1063 Validation Accuracy: 0.615400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.1180 Validation Accuracy: 0.612000\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.1419 Validation Accuracy: 0.594800\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.1043 Validation Accuracy: 0.620400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.0968 Validation Accuracy: 0.618600\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.1047 Validation Accuracy: 0.620600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.0972 Validation Accuracy: 0.618600\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.1724 Validation Accuracy: 0.592000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.0777 Validation Accuracy: 0.625800\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.0730 Validation Accuracy: 0.627600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.0837 Validation Accuracy: 0.631000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.1132 Validation Accuracy: 0.615000\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.0596 Validation Accuracy: 0.637400\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.0694 Validation Accuracy: 0.627000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.0470 Validation Accuracy: 0.641400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.0518 Validation Accuracy: 0.639200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.0767 Validation Accuracy: 0.627200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.0445 Validation Accuracy: 0.640200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.0549 Validation Accuracy: 0.640800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.0590 Validation Accuracy: 0.636800\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.0871 Validation Accuracy: 0.630800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.0865 Validation Accuracy: 0.627800\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.0934 Validation Accuracy: 0.620200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.0901 Validation Accuracy: 0.627800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.0488 Validation Accuracy: 0.639800\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.0354 Validation Accuracy: 0.653200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.0739 Validation Accuracy: 0.634800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.0475 Validation Accuracy: 0.648800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.0025 Validation Accuracy: 0.657800\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.0250 Validation Accuracy: 0.648800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.0384 Validation Accuracy: 0.654200\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.0543 Validation Accuracy: 0.643800\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.0446 Validation Accuracy: 0.644600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.0263 Validation Accuracy: 0.651800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.0662 Validation Accuracy: 0.635800\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.0265 Validation Accuracy: 0.653400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.1122 Validation Accuracy: 0.624600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.0399 Validation Accuracy: 0.650000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.0541 Validation Accuracy: 0.644000\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.0603 Validation Accuracy: 0.631400\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.0068 Validation Accuracy: 0.670000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.1392 Validation Accuracy: 0.632000\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.0272 Validation Accuracy: 0.663800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.0601 Validation Accuracy: 0.639800\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.0555 Validation Accuracy: 0.638800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.0053 Validation Accuracy: 0.670600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.0790 Validation Accuracy: 0.645000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.0433 Validation Accuracy: 0.656400\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.0423 Validation Accuracy: 0.653400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.0219 Validation Accuracy: 0.658800\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.0073 Validation Accuracy: 0.663800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.0371 Validation Accuracy: 0.662400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.0276 Validation Accuracy: 0.665600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.0256 Validation Accuracy: 0.654000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.0113 Validation Accuracy: 0.658200\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.0202 Validation Accuracy: 0.659800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.0508 Validation Accuracy: 0.662800\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.0408 Validation Accuracy: 0.658800\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.0145 Validation Accuracy: 0.659600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.0152 Validation Accuracy: 0.650600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.0229 Validation Accuracy: 0.670600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.0199 Validation Accuracy: 0.664200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.0272 Validation Accuracy: 0.665000\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.0665 Validation Accuracy: 0.660600\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.0318 Validation Accuracy: 0.661000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.0328 Validation Accuracy: 0.665200\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.5082 Validation Accuracy: 0.092800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.4754 Validation Accuracy: 0.126800\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.4485 Validation Accuracy: 0.130800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.4134 Validation Accuracy: 0.125800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.4093 Validation Accuracy: 0.128400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3700 Validation Accuracy: 0.153400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.3423 Validation Accuracy: 0.170400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.3246 Validation Accuracy: 0.181600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2983 Validation Accuracy: 0.213000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2626 Validation Accuracy: 0.233200\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2575 Validation Accuracy: 0.233000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.2519 Validation Accuracy: 0.233800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.2053 Validation Accuracy: 0.246400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.1990 Validation Accuracy: 0.252600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.1641 Validation Accuracy: 0.258600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.1401 Validation Accuracy: 0.264400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.1253 Validation Accuracy: 0.264400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.1142 Validation Accuracy: 0.269000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.0897 Validation Accuracy: 0.275800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.0762 Validation Accuracy: 0.275800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.0468 Validation Accuracy: 0.289000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.0367 Validation Accuracy: 0.291400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.0163 Validation Accuracy: 0.296400\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.9837 Validation Accuracy: 0.299800\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.9760 Validation Accuracy: 0.309600\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9428 Validation Accuracy: 0.316000\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.9543 Validation Accuracy: 0.310200\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.9278 Validation Accuracy: 0.315600\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.9158 Validation Accuracy: 0.322600\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.8998 Validation Accuracy: 0.331400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.8712 Validation Accuracy: 0.338600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.8748 Validation Accuracy: 0.331800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.8275 Validation Accuracy: 0.360000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.8251 Validation Accuracy: 0.356200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.8273 Validation Accuracy: 0.356800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8246 Validation Accuracy: 0.348800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.8039 Validation Accuracy: 0.364400\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.7797 Validation Accuracy: 0.368800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.7659 Validation Accuracy: 0.379600\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.7703 Validation Accuracy: 0.373400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.7720 Validation Accuracy: 0.373800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.7304 Validation Accuracy: 0.390600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.7243 Validation Accuracy: 0.389000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.7242 Validation Accuracy: 0.395000\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.7280 Validation Accuracy: 0.385400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7032 Validation Accuracy: 0.406400\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.6958 Validation Accuracy: 0.404400\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.6792 Validation Accuracy: 0.406000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.6869 Validation Accuracy: 0.408800\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.6711 Validation Accuracy: 0.414400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.6670 Validation Accuracy: 0.414200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.6599 Validation Accuracy: 0.414600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.6445 Validation Accuracy: 0.417600\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.6408 Validation Accuracy: 0.427600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.6446 Validation Accuracy: 0.419800\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.6146 Validation Accuracy: 0.427800\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.6156 Validation Accuracy: 0.432400\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.6067 Validation Accuracy: 0.434200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.6041 Validation Accuracy: 0.438000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.5947 Validation Accuracy: 0.433200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5708 Validation Accuracy: 0.447000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.5893 Validation Accuracy: 0.438800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.5728 Validation Accuracy: 0.444800\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.5784 Validation Accuracy: 0.440200\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.5684 Validation Accuracy: 0.445800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5574 Validation Accuracy: 0.457200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.5517 Validation Accuracy: 0.452600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.5425 Validation Accuracy: 0.454800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.5395 Validation Accuracy: 0.459400\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.5386 Validation Accuracy: 0.451200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5370 Validation Accuracy: 0.458200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.5301 Validation Accuracy: 0.464800\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.5190 Validation Accuracy: 0.466200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.5150 Validation Accuracy: 0.469400\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.5120 Validation Accuracy: 0.463200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.5106 Validation Accuracy: 0.466600\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.5094 Validation Accuracy: 0.462000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.4931 Validation Accuracy: 0.475200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.4887 Validation Accuracy: 0.471400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.4986 Validation Accuracy: 0.466400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.4855 Validation Accuracy: 0.471800\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.4825 Validation Accuracy: 0.477400\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.4778 Validation Accuracy: 0.472200\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.4849 Validation Accuracy: 0.483400\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.4768 Validation Accuracy: 0.476800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.4635 Validation Accuracy: 0.481400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.4587 Validation Accuracy: 0.481600\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.4619 Validation Accuracy: 0.479400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.4558 Validation Accuracy: 0.487400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.4555 Validation Accuracy: 0.483800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.4426 Validation Accuracy: 0.491000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.4533 Validation Accuracy: 0.490000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.4307 Validation Accuracy: 0.490800\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.4369 Validation Accuracy: 0.493800\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.4414 Validation Accuracy: 0.486600\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.4212 Validation Accuracy: 0.492400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.4222 Validation Accuracy: 0.493400\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.4144 Validation Accuracy: 0.502000\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.4196 Validation Accuracy: 0.496600\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.4241 Validation Accuracy: 0.493600\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     3.0248 Validation Accuracy: 0.113000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.8624 Validation Accuracy: 0.110000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.7663 Validation Accuracy: 0.110200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.6945 Validation Accuracy: 0.111800\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.6552 Validation Accuracy: 0.111600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.6238 Validation Accuracy: 0.111200\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.6073 Validation Accuracy: 0.110000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.5997 Validation Accuracy: 0.108800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.5877 Validation Accuracy: 0.109800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.5750 Validation Accuracy: 0.112000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.5604 Validation Accuracy: 0.107800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.5447 Validation Accuracy: 0.108000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.5448 Validation Accuracy: 0.108000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.5330 Validation Accuracy: 0.110600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.5265 Validation Accuracy: 0.109600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.5079 Validation Accuracy: 0.109800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.4971 Validation Accuracy: 0.109400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.4926 Validation Accuracy: 0.110800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.4796 Validation Accuracy: 0.110400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.4717 Validation Accuracy: 0.110400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.4611 Validation Accuracy: 0.112600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.4570 Validation Accuracy: 0.111400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.4514 Validation Accuracy: 0.112400\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     2.4440 Validation Accuracy: 0.113600\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     2.4397 Validation Accuracy: 0.114000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.4300 Validation Accuracy: 0.114800\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     2.4274 Validation Accuracy: 0.116600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     2.4227 Validation Accuracy: 0.115000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     2.4153 Validation Accuracy: 0.117200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     2.4138 Validation Accuracy: 0.118200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.4025 Validation Accuracy: 0.118800\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     2.3985 Validation Accuracy: 0.120000\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     2.3957 Validation Accuracy: 0.122200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     2.3906 Validation Accuracy: 0.122600\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     2.3877 Validation Accuracy: 0.123000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.3735 Validation Accuracy: 0.125600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     2.3683 Validation Accuracy: 0.124800\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     2.3667 Validation Accuracy: 0.126200\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     2.3583 Validation Accuracy: 0.126200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     2.3530 Validation Accuracy: 0.128000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.3409 Validation Accuracy: 0.129800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     2.3402 Validation Accuracy: 0.125800\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     2.3287 Validation Accuracy: 0.128200\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     2.3233 Validation Accuracy: 0.130400\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     2.3210 Validation Accuracy: 0.129200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.3069 Validation Accuracy: 0.132800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     2.3071 Validation Accuracy: 0.130000\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     2.3035 Validation Accuracy: 0.130600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     2.2937 Validation Accuracy: 0.136000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     2.2876 Validation Accuracy: 0.133200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.2735 Validation Accuracy: 0.136600\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     2.2823 Validation Accuracy: 0.133000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     2.2757 Validation Accuracy: 0.136600\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     2.2660 Validation Accuracy: 0.141200\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     2.2667 Validation Accuracy: 0.140600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.2527 Validation Accuracy: 0.142000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     2.2507 Validation Accuracy: 0.143200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     2.2446 Validation Accuracy: 0.144800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     2.2386 Validation Accuracy: 0.148000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     2.2398 Validation Accuracy: 0.150000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.2251 Validation Accuracy: 0.150600\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     2.2229 Validation Accuracy: 0.153400\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     2.2189 Validation Accuracy: 0.155600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     2.2168 Validation Accuracy: 0.157800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     2.2166 Validation Accuracy: 0.156400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     2.2051 Validation Accuracy: 0.162600\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     2.2047 Validation Accuracy: 0.163400\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     2.2096 Validation Accuracy: 0.161600\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     2.2038 Validation Accuracy: 0.160600\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     2.2025 Validation Accuracy: 0.161600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.1896 Validation Accuracy: 0.166400\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     2.1869 Validation Accuracy: 0.172400\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     2.1844 Validation Accuracy: 0.171600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     2.1790 Validation Accuracy: 0.176200\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     2.1787 Validation Accuracy: 0.174000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.1689 Validation Accuracy: 0.180600\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     2.1689 Validation Accuracy: 0.176800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     2.1714 Validation Accuracy: 0.179400\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     2.1607 Validation Accuracy: 0.180600\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     2.1635 Validation Accuracy: 0.187400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     2.1504 Validation Accuracy: 0.189000\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     2.1573 Validation Accuracy: 0.185200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     2.1590 Validation Accuracy: 0.186000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     2.1535 Validation Accuracy: 0.195200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     2.1491 Validation Accuracy: 0.191400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     2.1417 Validation Accuracy: 0.194400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     2.1402 Validation Accuracy: 0.199000\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     2.1437 Validation Accuracy: 0.197600\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     2.1351 Validation Accuracy: 0.204800\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     2.1306 Validation Accuracy: 0.205800\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     2.1206 Validation Accuracy: 0.211200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     2.1270 Validation Accuracy: 0.209000\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     2.1268 Validation Accuracy: 0.210200\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     2.1189 Validation Accuracy: 0.210200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     2.1176 Validation Accuracy: 0.214000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     2.1055 Validation Accuracy: 0.219400\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     2.1150 Validation Accuracy: 0.216600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     2.1155 Validation Accuracy: 0.213600\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     2.1049 Validation Accuracy: 0.223000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     2.1039 Validation Accuracy: 0.223200\n"
     ]
    }
   ],
   "source": [
    "#for batch_size in [512, 128, 32, 8, 2]:\n",
    "for learning_rate in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "\n",
    "    x, y, keep_prob, lr, summary, optimizer, cost, accuracy = build_op()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        steps = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Initializing the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # If you want to compare training and validation, one good way to do it is to use two separate\n",
    "        # file writer to keep logs for each process but keep them in the same folder. This way, you can \n",
    "        # later view them in the same plot. \n",
    "        #train_log_string = 'log/train, learning_rate={:.5f}, batch_size={}'.format(learning_rate, batch_size)\n",
    "        valid_log_string = 'log/valid, learning_rate={:.5f}, batch_size={}'.format(learning_rate, batch_size)\n",
    "        #train_filewriter = tf.summary.FileWriter(train_log_string, sess.graph)\n",
    "        valid_filewriter = tf.summary.FileWriter(valid_log_string, sess.graph)\n",
    "        \n",
    "        # Training cycle\n",
    "        for epoch in range(epochs):\n",
    "            # Loop over all batches\n",
    "            n_batches = 5\n",
    "            for batch_i in range(1, n_batches + 1):\n",
    "                for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                    steps += batch_features.shape[0]\n",
    "                    counter += batch_features.shape[0]\n",
    "                    sess.run(optimizer, feed_dict={x: batch_features, y: batch_labels,\n",
    "                                                   keep_prob: keep_probability, lr:learning_rate})\n",
    "                \n",
    "                    # Log only after the model is trained on every 500 samples. Getting summary takes time, so the \n",
    "                    # more frequently you look, the more extra time it'll cost you.\n",
    "                    if counter > 500:\n",
    "                        #train_summ = sess.run(summary, feed_dict={x: batch_features, y: batch_labels,\n",
    "                        #                                                   keep_prob: 1., lr:learning_rate})\n",
    "                        #train_filewriter.add_summary(train_summ, steps)\n",
    "                        valid_summ = sess.run(summary, feed_dict={x: valid_features, y: valid_labels,\n",
    "                                                                             keep_prob: 1., lr:learning_rate})\n",
    "                        \n",
    "                        valid_filewriter.add_summary(valid_summ, steps)\n",
    "                        counter -= 500\n",
    "                        \n",
    "                loss, valid_acc= sess.run([cost, accuracy], feed_dict={x: valid_features, y: valid_labels, \n",
    "                                                                        keep_prob: 1., lr:learning_rate})\n",
    "                \n",
    "                print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "                print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training logs in Tensorflow\n",
    "\n",
    "With the above code, after training begins, logs will be incrementally written to file on disk. You can actually monitor the ongoing training process. To visualize the logs, open another terminal and type in `tensorflow --logdir yourlogdir`. Here, `yourlogdir` is where you save the log files to. After tensorboard finds the log files, it'll give you an URL, which is very likely an equivalent of [localhost:6006](localhost:6006). Type this URL in your browser and you can visualize the training process ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_1_4]",
   "language": "python",
   "name": "conda-env-tf_1_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
